{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "A quick guide to `weight_formats` library usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**\n",
    "\n",
    "By convention, we use `import weight_formats.quantisation as Q`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=true\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_SILENT=true\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import weight_formats.analysis as A\n",
    "import weight_formats.experiments as E\n",
    "import weight_formats.fit as F\n",
    "import weight_formats.model_quantisation as M\n",
    "import weight_formats.quantisation as Q\n",
    "import weight_formats.sensitivity as S\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load a model and dataset (with calculation of reference output)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "model = E.RequantisableModel.load(\"meta-llama/Llama-3.2-1B\", DEVICE, torch.bfloat16)\n",
    "params = {k: v.detach() for k, v in model.model.named_parameters() if v.ndim == 2}\n",
    "\n",
    "# For a quick test - 1 batch of shape (16, 256)\n",
    "data = E.token_prediction.Dataset.load(model.model, sequence_length=256, batch_size=16, kl_topk=128, sequence_limit=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantise a single parameter, and evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.997058629989624 bpp\n",
      "R = tensor(0.0724)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cross_entropy': tensor([2.5975, 2.6224, 3.1826, 2.2801, 3.3747, 2.7373, 3.1326, 2.9880, 3.0249,\n",
       "         2.5361, 3.1598, 3.0744, 2.7587, 2.9885, 3.0168, 2.8383],\n",
       "        device='cuda:0'),\n",
       " 'kl_div': tensor([0.0010, 0.0010, 0.0009, 0.0008, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010,\n",
       "         0.0009, 0.0009, 0.0010, 0.0009, 0.0010, 0.0009, 0.0010],\n",
       "        device='cuda:0')}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.reset()\n",
    "\n",
    "p = model.model.state_dict()['model.layers.15.self_attn.v_proj.weight']\n",
    "fmt = Q.CompressedLUTFormat.train_grid(p, p.std().item() / 4)\n",
    "print(fmt.count_bits_tensor(p) / p.nelement(), \"bpp\")\n",
    "print(\"R =\", Q.qrmse_norm(fmt, p).cpu())\n",
    "p[...] = fmt.quantise(p)\n",
    "\n",
    "display(data.evaluate(model.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantise all parameters using `F.Scaled.fit`**\n",
    "\n",
    "This takes a few seconds, as it runs k-means per weight tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = M.quantise_2d_fixed(model.model, F.Scaled(4, \"lloyd_max\", Q.BFLOAT16, (1, 64), \"absmax\", compressor=None, args=dict(init=\"kmeans++\")))\n",
    "print(log[\"bits_per_param\"], \"bits/param\")\n",
    "display(log[\"params\"][\"model.layers.0.mlp.gate_proj.weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run a tiny sweep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    E.token_prediction.Baseline(),\n",
    "    E.token_prediction.QuantiseFixed(F.Scaled(4, \"fp\", Q.BFLOAT16, (1, 32), \"rms\", compressor=None)),\n",
    "]\n",
    "E.token_prediction.run_sweep([E.token_prediction.Run(\"dev\", test, E.core.MODELS[0]) for test in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 runs\n",
      "\n",
      "Last run:\n",
      "4.5006289 bits/param\n",
      "0.3004560172557831 KL\n",
      "2.523895025253296 X-Ent\n"
     ]
    }
   ],
   "source": [
    "runs = E.runs(\"dev\")\n",
    "print(len(runs), \"runs\")\n",
    "\n",
    "print(\"\\nLast run:\")\n",
    "log = runs[-1]\n",
    "print(log[\"summary\"][\"bits_per_param\"], \"bits/param\")\n",
    "print(torch.tensor(log[\"summary\"][\"kl_div\"]).mean().item(), \"KL\")\n",
    "print(torch.tensor(log[\"summary\"][\"cross_entropy\"]).mean().item(), \"X-Ent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
